---
title: "Week10 R Code - Anly 506-51"
author: "Nitin"
date: "July 23, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r pressure, echo=FALSE}

```

#K-means Cluster Analysis
Required libraries or packages for this exercise.
```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization

```

##Data Preparation
### There are three importent steps in data preparation for cluster analysis in R
Step 1: Rows are observations (individuals) and columns are variables
Step 2: Any missing value in the data must be removed or estimated.
Step 3: The data must be standardized (i.e., scaled) to make variables comparable. 
Data set used for this exercise is built-in R data set of USArrests this data set contains statistics of arrests per 100,000 residents in USA.
Read given data. 
```{r}
df <- USArrests
```

Remove missing value from the data.

```{r}
df <- na.omit(df)
```

Use scale function to scaling/standardizing the data. 
```{r}
df <- scale(df)
head(df)
```

# Clustering Distance Measures
Various methods for distance measures are Euclidean and Manhattan distances.

```{r}
distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```
## K-Means Clustering
K-means Algorithm
Plot two clusters by definging centers = 2
```{r}
k2 <- kmeans(df, centers = 2, nstart = 25)
str(k2)
```

Print the results for k2

```{r}
k2
```

Use function fviz_cluster when there are more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA) and plot the data points.

```{r}
fviz_cluster(k2, data = df)
```

Another way is standard pairwise scatter plots can be used show  the clusters compared to the original variables.

```{r}
df %>%
  as_tibble() %>%
  mutate(cluster = k2$cluster,
         state = row.names(USArrests)) %>%
  ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) +
  geom_text()
```

Execute the code for different number of cluster to see the diffrence.

```{r}
k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

## Determining Optimal Clusters
To determin optinal number of cluster we use three methods.
1. Elbow method
2. Silhouette method
3. Gap statistic
Randomize data with set.seed(123)
Use Elbow Method to calculate optinal number of cluster.
```{r}
set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(df, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```

```{r}
set.seed(123)

fviz_nbclust(df, kmeans, method = "wss")
```

Use Silhouette method to calculate optinal number of cluster.
```{r}
# function to compute average silhouette for k clusters
avg_sil <- function(k) {
  km.res <- kmeans(df, centers = k, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(df))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15

# extract avg silhouette for 2-15 clusters
avg_sil_values <- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")
```

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")
```

Use Gap statistic method to calculate optinal number of cluster.
```{r}
# compute gap statistic
set.seed(123)
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```
Use fuction fviz_gap_stat to visualize the optimal number of clusters.
```{r}
fviz_gap_stat(gap_stat)
```

```{r}
# Compute k-means clustering with k = 4
set.seed(123)
final <- kmeans(df, 4, nstart = 25)
print(final)
```

Use function fviz_cluster to visualize the results.
```{r}
fviz_cluster(final, data = df)
```

Descriptive statistics at the cluster level can be done by extracting the cluster and adding to initial data.
```{r}
USArrests %>%
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")
```

#Hierarchical Cluster Analysis
Hierarchical clustering can be divided into two main types first one is agglomerative and  second one is divisive.
Load required libraries and packages to these exercises.
```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
```

##Data Preparation
Read given data.
```{r}
df <- USArrests
```

Remove missing data.
```{r}
df <- na.omit(df)
```

Use scale function to scaling/standardizing the data. 
```{r}
df <- scale(df)
head(df)
```
### Agglomerative Hierarchical Clustering
```{r}
# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
```

```{r}
# Compute with agnes
hc2 <- agnes(df, method = "complete")

# Agglomerative coefficient
hc2$ac
```

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac)
```

```{r}
hc3 <- agnes(df, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
```

### Divisive Hierarchical Clustering

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(df)

# Divise coefficient; amount of clustering structure found
hc4$dc
## [1] 0.8514345

# plot dendrogram
pltree(hc4, cex = 0.6, hang = -1, main = "Dendrogram of diana")
```
### Working with Dendrograms
```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
sub_grp <- cutree(hc5, k = 4)

# Number of members in each cluster
table(sub_grp)
```

```{r}
USArrests %>%
  mutate(cluster = sub_grp) %>%
  head
```

```{r}
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 4, border = 2:5)
```

```{r}
fviz_cluster(list(data = df, cluster = sub_grp))
```

```{r}
# Cut agnes() tree into 4 groups
hc_a <- agnes(df, method = "ward")
cutree(as.hclust(hc_a), k = 4)

# Cut diana() tree into 4 groups
hc_d <- diana(df)
cutree(as.hclust(hc_d), k = 4)
```
Use function tanglegram to plot two dendrograms, side by side, with their labels connected by lines.
```{r}
# Compute distance matrix
res.dist <- dist(df, method = "euclidean")

# Compute 2 hierarchical clusterings
hc1 <- hclust(res.dist, method = "complete")
hc2 <- hclust(res.dist, method = "ward.D2")

# Create two dendrograms
dend1 <- as.dendrogram (hc1)
dend2 <- as.dendrogram (hc2)
library(dendextend)

tanglegram(dend1, dend2)
```

```{r}
dend_list <- dendlist(dend1, dend2)

tanglegram(dend1, dend2,
  highlight_distinct_edges = FALSE, # Turn-off dashed lines
  common_subtrees_color_lines = FALSE, # Turn-off line colors
  common_subtrees_color_branches = TRUE, # Color common branches 
  main = paste("entanglement =", round(entanglement(dend_list), 2))
  )
```
## Determining Optimal Clusters
Elbow Method
```{r}
fviz_nbclust(df, FUN = hcut, method = "wss")
```
Average Silhouette Method
```{r}
fviz_nbclust(df, FUN = hcut, method = "silhouette")
```
Gap Statistic Method
```{r}
gap_stat <- clusGap(df, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

